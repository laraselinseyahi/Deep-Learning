{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraselinseyahi/Diabetic-Retinopathy-Classification-using-Deep-Learning/blob/main/CS230_Project_LaraSelinSeyahi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jEmKnAODTe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ2QPTZPLRf9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ove3McH6M_Wx"
      },
      "outputs": [],
      "source": [
        "# upload the datafiles to drive\n",
        "\n",
        "%cd drive/MyDrive/retino\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing\n"
      ],
      "metadata": {
        "id": "FAdav1nhRF-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0HnmpHkOGBZ"
      },
      "outputs": [],
      "source": [
        "train_dir = 'train/'\n",
        "val_dir = 'valid/'\n",
        "test_dir = 'test/'\n",
        "\n",
        "# Image sizes\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "batch_size = 32\n",
        "\n",
        "# Training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg3oXXyjwJ6I"
      },
      "outputs": [],
      "source": [
        "# helps keep memory in cache after it's been loaded from the disk\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAlNzslLtOqo"
      },
      "outputs": [],
      "source": [
        "# image pre-processing\n",
        "normalization_layer = tf.keras.applications.resnet.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiiplEEZMtHz"
      },
      "outputs": [],
      "source": [
        "data_augmentation_layer = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50\n"
      ],
      "metadata": {
        "id": "PgAucb60eSAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OQbPO9t3uDF"
      },
      "outputs": [],
      "source": [
        "# RESNET baseline model\n",
        "\n",
        "# pre-trained model on imagenet\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# on top of the base model, add layers for binary classification\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))  # the input shape\n",
        "x = data_augmentation_layer(inputs)  # data augmentation\n",
        "x = normalization_layer(x)  # normalization (preprocessing)\n",
        "x = resnet_model(x)  # passing through the base ResNet model\n",
        "x = layers.GlobalAveragePooling2D()(x)  # average pooling layer\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)  # sigmoid for binary classification\n",
        "\n",
        "# create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS6rinWSAZGz"
      },
      "outputs": [],
      "source": [
        "# RESNET Experiment C\n",
        "\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation_layer(inputs)\n",
        "x = normalization_layer(x)\n",
        "x = resnet_model(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)  # dropout for regularization\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the top layers?\n",
        "resnet_model_top = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model_top.layers[-2:]:\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "KqudeGpWcMw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av6Ier-33z8I"
      },
      "outputs": [],
      "source": [
        "# RESNET50 - Training 50 epochs and unfreeze last 10 layers\n",
        "\n",
        "# delete top layer (include_top = false, make other layers non-trainable)\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation_layer(inputs)\n",
        "x = normalization_layer(x)\n",
        "x = resnet_model(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get the true labels for the test dataset\n",
        "y_true = np.concatenate([labels.numpy() for _, labels in test_ds])\n",
        "y_pred = model.predict(test_ds)  # Predicted probabilities\n",
        "y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "y_pred_classes = np.argmax(y_pred_labels, axis=1)  # Convert to class labels\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_labels)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for Binary Classification\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "85pckFkELxSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet"
      ],
      "metadata": {
        "id": "1iRwsvShRYBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2m-CArgvW0zW"
      },
      "outputs": [],
      "source": [
        "# ALEXNET Data Pre-Processing Baseline Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations: resizing, normalization ...\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for AlexNet\n",
        "])\n",
        "\n",
        "# train data\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# valid data\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Baseline Model\n",
        "\n",
        "# load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# freeze early layers\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# modify the classifier for binary classification\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 2)\n",
        "\n",
        "# move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # validation phase\n",
        "    alexnet.eval()  # set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "opU_H1vkh_g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Modified Baseline\n",
        "\n",
        "# load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# freeze early layers\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # Add dense layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # validation phase\n",
        "    alexnet.eval()  # set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "erqruonlUPs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Experiment A\n",
        "\n",
        "# load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(128, 128),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(128, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # validation phase\n",
        "    alexnet.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "w__8MWow0t-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Experiment B\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 256),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(256, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "htX33STw1KUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Experiment C\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # dense layer with 128 unit outputs\n",
        "    nn.ReLU(), # relu activation\n",
        "    nn.Dropout(p=0.5),  # dropout with a probability of 0.5\n",
        "    nn.Linear(128, 2)  # final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # validation phase\n",
        "    alexnet.eval()  # set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "pbGSVaX70463"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InceptionNet"
      ],
      "metadata": {
        "id": "ZW0DMvfmRt_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkY2TVEhF251"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET - Baseline Model\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),\n",
        "    data_augmentation_layer,\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "    inceptionnet_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9niQ-sHWV1"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET Experiment with drop out 1\n",
        "\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),\n",
        "\n",
        "    data_augmentation_layer,\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "\n",
        "    inceptionnet_model,\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEg9fu4xj3fz"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET - Experiment with drop out 2\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),  # Input size\n",
        "    data_augmentation_layer,\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "    inceptionnet_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX0EN_BlFzZ5"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET - Experiment with more data augmentation layers\n",
        "\n",
        "data_augmentation_more_layers = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2)\n",
        "])\n",
        "\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),\n",
        "    data_augmentation_more_layers,\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "\n",
        "    inceptionnet_model,\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}