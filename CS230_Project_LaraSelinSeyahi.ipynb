{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraselinseyahi/Deep-Learning/blob/main/CS230_Project_LaraSelinSeyahi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ2QPTZPLRf9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ove3McH6M_Wx"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/retino\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jEmKnAODTe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing\n"
      ],
      "metadata": {
        "id": "FAdav1nhRF-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0HnmpHkOGBZ"
      },
      "outputs": [],
      "source": [
        "train_dir = 'train/'\n",
        "val_dir = 'valid/'\n",
        "test_dir = 'test/'\n",
        "\n",
        "# Define image parameters\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "batch_size = 32\n",
        "\n",
        "# Load training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Load test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False  # Important for evaluation; keep order consistent\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg3oXXyjwJ6I"
      },
      "outputs": [],
      "source": [
        "# helps keep memory in cache after it's been loaded from the disk\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAlNzslLtOqo"
      },
      "outputs": [],
      "source": [
        "# normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "normalization_layer = tf.keras.applications.resnet.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiiplEEZMtHz"
      },
      "outputs": [],
      "source": [
        "data_augmentation_layer = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "def data_augmentation_layer_function(inputs):\n",
        "    return data_augmentation_layer(inputs)\n",
        "\n",
        "# Apply the data augmentation to each element in the dataset\n",
        "def apply_data_augmentation(image, label):\n",
        "    # Apply augmentation only to the image (not the label)\n",
        "    augmented_image = data_augmentation_layer(image)\n",
        "    return augmented_image, label\n",
        "\n",
        "# Define the normalization layer\n",
        "def normalization_layer(inputs):\n",
        "    return tf.keras.applications.resnet.preprocess_input(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50\n"
      ],
      "metadata": {
        "id": "PgAucb60eSAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OQbPO9t3uDF"
      },
      "outputs": [],
      "source": [
        "# RESNET\n",
        "\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))  # Define the input shape\n",
        "x = data_augmentation_layer(inputs)  # Apply data augmentation\n",
        "x = normalization_layer(x)  # Apply normalization (preprocessing)\n",
        "x = resnet_model(x)  # Pass through the base ResNet model\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS6rinWSAZGz"
      },
      "outputs": [],
      "source": [
        "# RESNET Experimenting\n",
        "\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))  # Define the input shape\n",
        "x = data_augmentation_layer(inputs)  # Apply data augmentation\n",
        "x = normalization_layer(x)  # Apply normalization (preprocessing)\n",
        "x = resnet_model(x)  # Pass through the base ResNet model\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer\n",
        "x = layers.Dense(128, activation='relu')(x)  # Dense layer\n",
        "x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification # try with having on this dense layer! then add one by one\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av6Ier-33z8I"
      },
      "outputs": [],
      "source": [
        "# RESNET (Training 50 epochs and unfroze last 10 layers)\n",
        "\n",
        "# delete top layer (include_top = false, make other layers non-trainable)\n",
        "resnet_model_top = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model_top.layers[-2:]:\n",
        "  print(layer)\n",
        "\n",
        "\n",
        "# delete top layer (include_top = false, make other layers non-trainable)\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "#x = normalization_layer(x)\n",
        "\n",
        "# x = train_ds.map(apply_data_augmentation)\n",
        "# x = normalization_layer(train_ds)\n",
        "\n",
        "# on top of the base model, add layers for binary classification\n",
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))  # Define the input shape\n",
        "x = data_augmentation_layer(inputs)  # Apply data augmentation\n",
        "x = normalization_layer(x)  # Apply normalization (preprocessing)\n",
        "x = resnet_model(x)  # Pass through the base ResNet model\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer # TRY OUT different ways!! # hypothesis (ex: same performance can be achieved with less layers, reduce the number of layers)\n",
        "x = layers.Dense(128, activation='relu')(x)  # Dense layer\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification # try with having on this dense layer! then add one by one\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=50  # Start with a few epochs\n",
        ")\n",
        "\n",
        "\n",
        "# try not fine-tuning the model on the dataset below 94 (both accuracies)\n",
        "# usually you have a hypothesis about the architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data\n",
        "x = [i for i in range(1, 51)]  # X-axis values\n",
        "y = [0.8912, 0.9646, 0.9677, 0.9779, 0.9759, 0.9713, 0.9771, 0.9827, 0.9828, 0.9842, 0.9902, 0.9818, 0.9856, 0.9871, 0.9890, 0.9886, 0.9931, 0.9752, 0.9921, 0.9836, 0.9907, 0.9905, 0.9924, 0.9872, 0.9958, 0.9889, 0.9922, 0.9899, 0.9923, 0.9908, 0.9918, 0.9929, 0.9954, 0.9927, 0.9952, 0.9961, 0.9941, 0.9962, 0.9968, 0.9914, 0.9930, 0.9950, 0.9962, 0.9966, 0.9925, 0.9947, 0.9966, 0.9973, 0.9951, 0.9983]  # Y-axis values\n",
        "print(len(y))\n",
        "y = list(map(lambda x: x * 100, y))\n",
        "\n",
        "plt.figure(figsize=(15, 5))  # Width = 15, Height = 5\n",
        "val_acc = [0.9412, 0.9091, 0.9269, 0.8574, 0.9109, 0.9715, 0.9715, 0.9715, 0.9661, 0.9733, 0.9679, 0.9768, 0.9412, 0.9733, 0.9679, 0.9733, 0.9733, 0.9590, 0.9697, 0.9715, 0.9643, 0.9572, 0.9715, 0.9750, 0.9715, 0.9679, 0.9643, 0.9572, 0.8289, 0.9733, 0.9715, 0.9768, 0.9733, 0.9697, 0.9697, 0.9840, 0.9840, 0.9733, 0.9715, 0.9733, 0.9750, 0.9750, 0.9804, 0.9750, 0.9768, 0.9786, 0.9786, 0.9804, 0.9822, 0.9715]\n",
        "val_acc = list(map(lambda x: x * 100, val_acc))\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(x, y, label=\"Training Accuracy\", color=\"blue\", marker=\"o\")\n",
        "plt.plot(x, val_acc, label=\"Validation Accuracy\", color=\"red\", marker=\"o\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"# of Epoch\")\n",
        "plt.ylabel(\"Accuracy %\")\n",
        "plt.title(\" Accuracies Across Epochs\")\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(ticks=range(1, 51, 1))  # Show a tick every 2 numbers\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z7KTMSE9Ct7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet"
      ],
      "metadata": {
        "id": "1iRwsvShRYBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2m-CArgvW0zW"
      },
      "outputs": [],
      "source": [
        "# ALEXNET Baseline Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (resizing, normalization, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for AlexNet\n",
        "])\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#should I instead write it from scratch with tensor flow?\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier for binary classification\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 2) # should I add av pooling, relu and sigmoid?\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET Experiments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (resizing, normalization, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for AlexNet\n",
        "])\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#should I instead write it from scratch with tensor flow?\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # Add dense layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "erqruonlUPs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (resizing, normalization, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for AlexNet\n",
        "])\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#should I instead write it from scratch with tensor flow?\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(128, 128),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(128, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "w__8MWow0t-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Defining transformations (resizing, normalization, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for AlexNet\n",
        "])\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 128),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Dropout(p=0.5),  # Dropout with a probability of 0.5\n",
        "    nn.Linear(128, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "pbGSVaX70463"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALEXNET !!!\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (resizing, normalization, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize the image to 256x256\n",
        "    transforms.CenterCrop(224),   # Crop the center of the image to 224x224\n",
        "    transforms.ToTensor(),        # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for AlexNet\n",
        "])\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load dataset (for example, using ImageFolder)\n",
        "valid_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Freeze early layers (optional)\n",
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    *list(alexnet.classifier.children())[:-1],  # Remove the last layer\n",
        "    nn.Linear(alexnet.classifier[6].in_features, 256),  # Add dense layer\n",
        "    nn.ReLU(),  # ReLU activation\n",
        "    nn.Linear(256, 2)  # Final output layer for 2 classes\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print statistics for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    alexnet.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update validation metrics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "htX33STw1KUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InceptionNet"
      ],
      "metadata": {
        "id": "ZW0DMvfmRt_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEg9fu4xj3fz"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Apply data augmentation to the input\n",
        "    data_augmentation,\n",
        "    # Resize the images to 299x299\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    # Preprocess the image data using InceptionV3 preprocessing\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "\n",
        "    inceptionnet_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "\n",
        "# train_ds = train_ds.map(lambda x, y: (tf.keras.applications.inception_v3.preprocess_input(data_augmentation(x)), y))\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# sincd this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5  # Start with a few epochs\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX0EN_BlFzZ5"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET !!\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2)\n",
        "])\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),  # Input can have dynamic size; adjust as needed\n",
        "\n",
        "    # Apply data augmentation to the input\n",
        "    data_augmentation,\n",
        "    # Resize the images to 299x299\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    # Preprocess the image data using InceptionV3 preprocessing\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "\n",
        "    inceptionnet_model,\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "\n",
        "# train_ds = train_ds.map(lambda x, y: (tf.keras.applications.inception_v3.preprocess_input(data_augmentation(x)), y))\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# sincd this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5  # Start with a few epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9niQ-sHWV1"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),  # Input can have dynamic size; adjust as needed\n",
        "\n",
        "    # Apply data augmentation to the input\n",
        "    data_augmentation,\n",
        "    # Resize the images to 299x299\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    # Preprocess the image data using InceptionV3 preprocessing\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "\n",
        "    inceptionnet_model,\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# sincd this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5  # Start with a few epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dafaHGNbG8Bq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkY2TVEhF251"
      },
      "outputs": [],
      "source": [
        "# INCEPTIONNET\n",
        "\n",
        "# Load InceptionNet without the top layer\n",
        "inceptionnet_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "for layer in inceptionnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(299, 299, 3)),  # Input can have dynamic size; adjust as needed\n",
        "    # Apply data augmentation to the input\n",
        "    data_augmentation,    # Resize the images to 299x299\n",
        "    layers.Lambda(lambda x: tf.image.resize(x, (299, 299))),\n",
        "    # Preprocess the image data using InceptionV3 preprocessing\n",
        "    layers.Lambda(lambda x: tf.keras.applications.inception_v3.preprocess_input(x)), # normalization\n",
        "    inceptionnet_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# train_ds = train_ds.map(lambda x, y: (tf.keras.applications.inception_v3.preprocess_input(data_augmentation(x)), y))\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "# sincd this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train with frozen base model layers\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7vCn_-QIzwd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}