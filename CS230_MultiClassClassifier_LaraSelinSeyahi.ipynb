{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraselinseyahi/Diabetic-Retinopathy-Classification-using-Deep-Learning/blob/main/CS230_MultiClassClassifier_LaraSelinSeyahi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass Classification with ResNet-50 for Diabetic Retinopathy"
      ],
      "metadata": {
        "id": "rgYkEukKSHGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ2QPTZPLRf9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ove3McH6M_Wx"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jEmKnAODTe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'diabetic_retinopathy'\n",
        "\n",
        "# Class definitions\n",
        "classes = [\"healthy\", \"mild_dr\", \"moderate_dr\", \"proliferate_dr\", \"severe_dr\"]\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "# creates the following dict {\"healthy\": 0, \"mild_dr\": 1, \"moderate_dr\": 2, \"proliferate_dr\": 3, \"severe_dr\": 4}\n",
        "\n"
      ],
      "metadata": {
        "id": "qtVwPqh3kf8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# TensorFlow works efficiently with datasets when file paths and labels are paired,\n",
        "# as it can process the data in batches and apply preprocessing.\n",
        "# This ensures that each image is correctly paired with its label, which is crucial for supervised learning.\n",
        "# This part collects all the image file paths and their corresponding labels.\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(data_dir, cls)\n",
        "    for file in os.listdir(class_dir):\n",
        "        file_paths.append(os.path.join(class_dir, file))\n",
        "        labels.append(class_to_idx[cls])\n",
        "\n",
        "# Split the dataset into train, validation, and test sets (70% train, remaining eval&test)\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    file_paths, labels, test_size=0.3, stratify=labels, random_state=42\n",
        ")\n",
        "# 20% eval, 10% test\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.33, stratify=temp_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Sanity check\n",
        "print(f\"Train: {len(train_paths)}, Validation: {len(val_paths)}, Test: {len(test_paths)}\")\n",
        "\n",
        "# Define image size and batch size\n",
        "IMG_SIZE = (224, 224) # ResNet-50 input size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(file_path, label):\n",
        "    # Load the image\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    # Perform data augmentation for labels not in [0, 2]\n",
        "    if label == 1 or label == 3 or label == 4:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_flip_up_down(image)\n",
        "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "        image = tf.image.random_crop(image, size=(int(IMG_SIZE[0] * 0.9), int(IMG_SIZE[1] * 0.9), 3))\n",
        "        image = tf.image.resize(image, IMG_SIZE)  # Resize back to original size after cropping\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image) #ResNet-50's requirement of [-1, 1]\n",
        "    return image, label\n",
        "\n",
        "# Create a TensorFlow dataset\n",
        "def create_dataset(file_paths, labels, shuffle=True):\n",
        "    # pair file paths and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "    # map function applies preprocess_image function to each image\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(file_paths))\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "train_dataset = create_dataset(train_paths, train_labels)\n",
        "val_dataset = create_dataset(val_paths, val_labels, shuffle=False)\n",
        "test_dataset = create_dataset(test_paths, test_labels, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "vrNBQLDF66XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESNET Experimenting\n",
        "\n",
        "# RESNET\n",
        "\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Add a new top layer for our 5-class classification\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(), # pooling to reduce output size\n",
        "    tf.keras.layers.Dense(5, activation='softmax')  # 5 classes\n",
        "\n",
        "    # adding more layer increases the ability of your model, but the model is already good\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,  # Adjust based on your needs\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        # Early stopping for stopping training when validation performance stops improving\n",
        "        tf.keras.callbacks.ModelCheckpoint('resnet50_best_model.keras', save_best_only=True)\n",
        "        # saves best performing model during training\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "At6mUocK8PNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESNET Experimenting\n",
        "\n",
        "# RESNET\n",
        "\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "for layer in base_model.layers[-10:]: # unfreezing last 10 layers\n",
        "  layer.trainable = True\n",
        "\n",
        "# Add a new top layer for our 5-class classification\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(), # pooling to reduce output size\n",
        "    tf.keras.layers.Dense(5, activation='softmax')  # 5 classes\n",
        "\n",
        "    # adding more layer increases the ability of your model, but the model is already good\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,  # Adjust based on your needs\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ModelCheckpoint('resnet50_best_model.keras', save_best_only=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bxwNHLYDflq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OQbPO9t3uDF"
      },
      "outputs": [],
      "source": [
        "# RESNET\n",
        "\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Add a new top layer for our 5-class classification\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(), # pooling to reduce output size\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5), # regularization\n",
        "    tf.keras.layers.Dense(5, activation='softmax')  # 5 classes\n",
        "])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "# since this is binary classification, loss is BCE\n",
        "model.compile(optimizer=Adam(learning_rate=base_learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,  # Adjust based on your needs\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        # Early stopping for stopping training when validation performance stops improving\n",
        "        tf.keras.callbacks.ModelCheckpoint('resnet50_best_model.keras', save_best_only=True)\n",
        "        # saves best performing model during training\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}